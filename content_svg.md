This are my notes and observations from reading the [Linear Algebra chapter](http://www.deeplearningbook.org/contents/linear_algebra.html) of the [Deep Learning](http://www.deeplearningbook.org/) book.

The following notes are presented in order of value to the reader. I start with a discussion of the Moore-Penrose pseudoinverse, followed by a short reflection on "broadcasting".

## Moore-Penrose pseudoinverse

The Moore-Penrose pseudoinverse is defined as:

> [<p align="center"><img alt="$$ A^{+}:=\lim _{\alpha \searrow  0}\left( A^{T}A+\alpha I\right) ^{-1}A^{T} $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/e192aca6b2355d51d363db663569309a.svg?invert_in_darkmode" align=middle width="211.99199999999996pt" height="30.933870000000002pt"/></p>](http://www.deeplearningbook.org/contents/linear_algebra.html#pff)

You can click on the formula to jump into the book. (It's really awesome that all of it is online and you can deep link to pages.)

This formula is mystifying. What is its origin? And why does it do what the book says it does?

We can get a feeling for its origin by looking at the equation <img alt="$Ax=b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width="50.540985000000006pt" height="22.745910000000016pt"/> that it solves:

<p align="center"><img alt="$$&#10;\begin{align*} Ax &amp;=b &amp;|&amp; A^{T}\cdot \\ A^{T}Ax &amp;=A^{T}b &amp;|&amp; \left( A^{T}A\right) ^{-1} \cdot \\ \left( A^{T}A\right) ^{-1}A^{T}Ax &amp;=\left( A^{T}A\right) ^{-1}A^{T}b &amp; &amp; \\ x &amp;=\left( A^{T}A\right) ^{-1}A^{T}b  &amp; &amp; \end{align*}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/22f606bae448a91e54d360dcaaf1980a.svg?invert_in_darkmode" align=middle width="335.7684pt" height="111.78865499999999pt"/></p>

So we start with the equation that we want to solve and find a more complex form for solving for <img alt="$x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width="9.359955000000003pt" height="14.102549999999994pt"/> than the usual left-multiplication with the inverse <img alt="$A^{-1}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/471d65ea6d03a4f1ea1dd8be931d26c9.svg?invert_in_darkmode" align=middle width="29.046435000000002pt" height="26.70657pt"/>. This is already useful if <img alt="$A^T A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/754dfb06e94eb52ca31296f2efb3ecf0.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> is invertible and <img alt="$A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width="12.282765000000003pt" height="22.381919999999983pt"/> is not square (because matrix inverses only exist for square matrices). 

This already looks similar to the pseudo-inverse, except for the <img alt="$+\alpha I$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/3c90e739d33257f04a0bcd439a590164.svg?invert_in_darkmode" align=middle width="31.759035pt" height="22.381919999999983pt"/> term. 

### Aside: there is also a right pseudo-inverse
We can guess its form by starting with:

<p align="center"><img alt="$$&#10;\begin{align*} yA&amp;=b&amp;|&amp;\cdot A^{T}\\ yAA^{T}&amp;=bA^{T}&amp;|&amp;\cdot \left( AA^{T}\right) ^{-1}\\ yAA^{T}\left( AA^{T}\right) ^{-1}&amp;=bA^{T}\left( AA^{T}\right) ^{-1}\\ y&amp;=bA^{T}\left( AA^{T}\right) ^{-1}\end{align*}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/751edde61d407981c7d63aed1fdd239e.svg?invert_in_darkmode" align=middle width="335.9631pt" height="111.78865499999999pt"/></p>

From this, we can make an educated guess:

<p align="center"><img alt="$$&#10;^{+}A = A^{T}\lim _{\alpha \searrow 0}\left( AA^{T}+\alpha I\right) ^{-1}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/2598914986dfe59a0b7879f5e2218650.svg?invert_in_darkmode" align=middle width="207.41654999999997pt" height="30.933870000000002pt"/></p>

Since we cannot always invert <img alt="$A^TA$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> (respectively <img alt="$AA^T$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/d4f5cd9596d7bf6138dc0de701a2ffc1.svg?invert_in_darkmode" align=middle width="34.06359pt" height="27.598230000000008pt"/>), an obvious question is:

### Why can we invert <img alt="$AA^T+ \alpha I$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/dca34c25fcaada78fdcbc2dd8ef74e71.svg?invert_in_darkmode" align=middle width="73.986pt" height="27.598230000000008pt"/> for positive <img alt="$\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width="10.537065000000004pt" height="14.102549999999994pt"/>?

Let's examine this. [For a matrix to be invertible](https://en.wikipedia.org/wiki/Invertible_matrix), its kernel has to only contain the zero vector:

<p align="center"><img alt="$$\ker \left( A^{T}A+\alpha I\right) =\left\{ 0\right\}$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/668ed61c3267b0419d05f1b984b9cdf8.svg?invert_in_darkmode" align=middle width="160.43461499999998pt" height="20.30325pt"/></p>

To prove that this is the case for <img alt="$\left( AA^{T}+\alpha I\right)$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/0e247bfcfb86f59ea815037a772d2f16.svg?invert_in_darkmode" align=middle width="89.031195pt" height="27.940769999999983pt"/>, we need to show that:

<p align="center"><img alt="$$  \left( A^{T}A+\alpha I\right) v = 0 \implies v = 0$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/044cfca8414defb2d52a9320abd5b0b1.svg?invert_in_darkmode" align=middle width="213.8169pt" height="20.30325pt"/></p>

So starting with the left side, we can rephrase it as follows:

<p align="center"><img alt="$$ \begin{align*} &#10;&amp; &amp; \left( A^{T}A+\alpha I\right) v&amp;= 0\\ &#10;&amp; \Leftrightarrow &amp;A^{T}Av+\alpha Iv&amp;= 0\\ &#10;&amp; \Leftrightarrow &amp;A^{T}Av &amp;= -\alpha v&#10;\end{align*} $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/53af8bf880045d8460d9571bf682e551.svg?invert_in_darkmode" align=middle width="187.10999999999999pt" height="70.27267499999999pt"/></p>

If <img alt="$v \ne 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/2080a69cb2bf5bdaddca1c09b72eae98.svg?invert_in_darkmode" align=middle width="38.586405000000006pt" height="22.745910000000016pt"/>, this would mean that <img alt="$-\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/18f225af1d6f8f01f3893d9c0d2ea918.svg?invert_in_darkmode" align=middle width="23.274735000000003pt" height="19.10667000000001pt"/> is a negative [eigenvalue](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of <img alt="$A^TA$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> as <img alt="$\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width="10.537065000000004pt" height="14.102549999999994pt"/> is <img alt="$&gt;0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/ea4bbf715156e61bd05c0ec553601019.svg?invert_in_darkmode" align=middle width="25.492335000000004pt" height="21.10812pt"/>.

#### Can <img alt="$A^TA$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> have negative eigenvalues? 

Let's assume <img alt="$v \ne 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/2080a69cb2bf5bdaddca1c09b72eae98.svg?invert_in_darkmode" align=middle width="38.586405000000006pt" height="22.745910000000016pt"/> and <img alt="$-\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/18f225af1d6f8f01f3893d9c0d2ea918.svg?invert_in_darkmode" align=middle width="23.274735000000003pt" height="19.10667000000001pt"/> is a negative eigenvalue, that is <img alt="$A^{T}Av = -\alpha v$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/5b4addac87a0794950b3acb4838498f3.svg?invert_in_darkmode" align=middle width="97.11982499999999pt" height="27.598230000000008pt"/> holds (and <img alt="$\alpha &gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/027fadd6000dcab00d8ea8a8007ce450.svg?invert_in_darkmode" align=middle width="40.59561pt" height="21.10812pt"/>). We can left-multiply with <img alt="$v^T$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/7d9548e844afbfdd37b08e2f4fffc222.svg?invert_in_darkmode" align=middle width="18.026250000000005pt" height="27.598230000000008pt"/> and obtain:

<p align="center"><img alt="$$ &#10;\begin{align*} &amp; \Leftrightarrow v^{T}A^{T}Av=v^{T}\left( -\alpha v\right) \\ &amp; \Leftrightarrow \left\| Av\right\| _{2}^{2}=-\alpha v^{T}v=-\alpha \left\| v\right\| _{2}^{2}\\ &amp; \Leftrightarrow -\alpha =\dfrac {\left\| Av\right\| _{2}^{2}} {\left\| v\right\| _{2}^{2}}\geq 0&#10;\end{align*} &#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/ae53ff7fc136c652c4723912a43aea07.svg?invert_in_darkmode" align=middle width="217.60695pt" height="101.11398pt"/></p>

(We can divide by <img alt="$\left\| v\right\| _{2}^{2}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/061855aa3afdd577118c6843c22ee7b0.svg?invert_in_darkmode" align=middle width="31.524405pt" height="31.305449999999972pt"/> because we assume <img alt="$v\ne0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/aa327f7cbe6ebd662e2f57e44b7c2e97.svg?invert_in_darkmode" align=middle width="38.586405000000006pt" height="22.745910000000016pt"/>.)
Now this means, that <img alt="$-\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/18f225af1d6f8f01f3893d9c0d2ea918.svg?invert_in_darkmode" align=middle width="23.274735000000003pt" height="19.10667000000001pt"/> has to be <img alt="$&gt;0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/ea4bbf715156e61bd05c0ec553601019.svg?invert_in_darkmode" align=middle width="25.492335000000004pt" height="21.10812pt"/>, so it is not a negative eigenvalue and a contradiction to our initial assumption <img alt="$\alpha &gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/027fadd6000dcab00d8ea8a8007ce450.svg?invert_in_darkmode" align=middle width="40.59561pt" height="21.10812pt"/>. In fact, we have just shown that <img alt="$A^TA$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> in general can only have non-negative eigenvalues. (We can show the same for <img alt="$AA^T$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/d4f5cd9596d7bf6138dc0de701a2ffc1.svg?invert_in_darkmode" align=middle width="34.06359pt" height="27.598230000000008pt"/> the same way.) 

Because <img alt="$A^TA$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> cannot have negative eigenvalues, this means that <img alt="$v$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6c4adbc36120d62b98deef2a20d5d303.svg?invert_in_darkmode" align=middle width="8.52588pt" height="14.102549999999994pt"/> cannot be <img alt="$\ne 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/51acea41aee76157ee6505d98e123acd.svg?invert_in_darkmode" align=middle width="25.492335000000004pt" height="22.745910000000016pt"/>, so <img alt="$v = 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/cf5e8f7f774bf3ef44db37ce2076876b.svg?invert_in_darkmode" align=middle width="38.586405000000006pt" height="21.10812pt"/>. This is what we wanted to show, and this means that the kernel only contains the zero vector, and <img alt="$AA^T + \alpha I$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/3d1a1c5c408a1ad7484ea96329019082.svg?invert_in_darkmode" align=middle width="73.986pt" height="27.598230000000008pt"/> is invertible for <img alt="$\alpha &gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/027fadd6000dcab00d8ea8a8007ce450.svg?invert_in_darkmode" align=middle width="40.59561pt" height="21.10812pt"/>.

### What if <img alt="$A^T A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/754dfb06e94eb52ca31296f2efb3ecf0.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> was invertible?

For a moment, let's consider the case when <img alt="$A^T A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/754dfb06e94eb52ca31296f2efb3ecf0.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> is invertible: Is the Moore-Penrose pseudoinverse then equal to <img alt="$\left (A^T A \right)^{-1} A^T$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c8db129c4935882692fed4e312739f05.svg?invert_in_darkmode" align=middle width="92.250675pt" height="34.59323999999999pt"/>?
 
Matrix inversion is continuous in the space of invertible matrices. You might remember the definition of continuity from school. [A better definition of continuity](https://en.wikipedia.org/wiki/Continuous_function#Definition_in_terms_of_limits_of_sequences) is that it means being able to swap function and limit application:

> A function <img alt="$f$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width="9.780705000000003pt" height="22.745910000000016pt"/> is continuous on its domain iff for any convergent series <img alt="$x_n \to x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/327a7e62c8aa1ca83e50902cba986408.svg?invert_in_darkmode" align=middle width="53.177025pt" height="14.102549999999994pt"/> for which <img alt="$f \left( x \right )$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/3b39248e4142cafe7eb56b8935dc0ea0.svg?invert_in_darkmode" align=middle width="34.648515pt" height="24.56552999999997pt"/> is defined
> <p align="center"><img alt="$$ \lim_{n \to \infty } f \left ( x_n \right ) = f \left ( \lim_{n \to \infty} x_n \right ) = f \left( x \right).$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/2964cab7c6196257a0a266df0718ccfb.svg?invert_in_darkmode" align=middle width="254.04719999999998pt" height="29.478734999999997pt"/></p> 

Assuming <img alt="$A^T A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/754dfb06e94eb52ca31296f2efb3ecf0.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> is invertible and using the fact that matrix inversion is continuous for invertible matrices, we now see:

<p align="center"><img alt="$$ \begin{align*} &#10;\lim _{\alpha \searrow 0}\left( A^{T} A +\alpha I\right) ^{-1} A^T &amp;= \left( \lim _{\alpha \searrow 0} A^{T} A +\alpha I\right) ^{-1} A^T \\&#10;&amp;= \left( A^{T}A+0 \, I\right) ^{-1} A^T \\&#10;&amp;= \left( A^{T}A\right) ^{-1} A^T&#10;\end{align*}  $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/9b3832e581662ab4a659b0dbce16601f.svg?invert_in_darkmode" align=middle width="356.66069999999996pt" height="103.531065pt"/></p>

So in this special case, the pseudoinverse is exactly the solution we have come up with ourselves.

### Two properties of the pseudoinverse

There are two properties mentioned in text that are interesting but not obvious:

> [When <img alt="$A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width="12.282765000000003pt" height="22.381919999999983pt"/> has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides the solution <img alt="$x=A^+y$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c2d404f4a018368632e8a1c741cfe830.svg?invert_in_darkmode" align=middle width="63.042705pt" height="26.124119999999984pt"/> with minimal Euclidean norm <img alt="$\left\| x \right\|_2$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/bf1214b14f1ef3bff26d7bf700e3c698.svg?invert_in_darkmode" align=middle width="32.361450000000005pt" height="24.56552999999997pt"/> among all possible solutions. When <img alt="$A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width="12.282765000000003pt" height="22.381919999999983pt"/> has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the <img alt="$x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width="9.359955000000003pt" height="14.102549999999994pt"/> for which <img alt="$Ax$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/bbb2565155df2f2e483c15107e8505b1.svg?invert_in_darkmode" align=middle width="21.64272pt" height="22.381919999999983pt"/> is as close as possible to <img alt="$y$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width="8.616960000000002pt" height="14.102549999999994pt"/> in terms of Euclidean norm <img alt="$\left\| Ax - y \right\|_2$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/dc9f9c6a697362e1e0062297abafc0ad.svg?invert_in_darkmode" align=middle width="73.43061pt" height="24.56552999999997pt"/>.](http://www.deeplearningbook.org/contents/linear_algebra.html#pf10)

These properties are not obvious and their deduction is enlightening towards the chosen definition of the pseudoinverse, specifically the use of the limit and the constraint of <img alt="$\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width="10.537065000000004pt" height="14.102549999999994pt"/> to be <img alt="$&gt;0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/ea4bbf715156e61bd05c0ec553601019.svg?invert_in_darkmode" align=middle width="25.492335000000004pt" height="21.10812pt"/>.

#### A related optimization problem

To prove the properties, let's look at the following regularized minimum-least-squares problem. This is something that is formulated in more detail in [Chapter 4 of the book](http://www.deeplearningbook.org/contents/numerical.html#pf11), but it is quite useful here:

<p align="center"><img alt="$$\min _{x}\left| \left| Ax-b\right| \right| _{2}^{2}+\alpha \left\| x\right\| _{2}^{2}, \, \alpha &gt; 0$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/7444d32df0509c8b69bab66d5b4ad2b9.svg?invert_in_darkmode" align=middle width="224.5386pt" height="25.515765pt"/></p>

Let's solve this optimization problem. First, we set up a cost function:

<p align="center"><img alt="$$ \begin{align*} &#10;c_\alpha\left( x\right) &amp;=\left\| Ax-b\right\| _{2}^{2}+\alpha \left\| x\right\| _{2}^{2}\\&#10;&amp; =\left( Ax-b\right) ^{T}\left( Ax-b\right) +\alpha x^{T}x\\&#10;&amp; =x^{T}AAx-2b^{T}Ax+b^{T}b+\alpha x^{T} x \end{align*} \\&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/291c34285f59fc99c39ffe381ce163a2.svg?invert_in_darkmode" align=middle width="288.28305pt" height="72.781995pt"/></p>

The first derivative of <img alt="$c_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/45300ff095a52c14741ae14e03807926.svg?invert_in_darkmode" align=middle width="15.601245pt" height="14.102549999999994pt"/> is:

<p align="center"><img alt="$$ \nabla c_\alpha\left( x\right) =2A^{T}Ax-2A^{T}b+2\alpha x$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/9d9f87b2b72cbd988bb61f6fa1388012.svg?invert_in_darkmode" align=middle width="235.38899999999998pt" height="18.715124999999997pt"/></p>

And the second derivative is:

<p align="center"><img alt="$$ H_\alpha \left ( x \right ) = 2A^T A + 2\alpha I$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/d9a10c438667fed2d74dca6b809e7798.svg?invert_in_darkmode" align=middle width="160.133985pt" height="18.715124999999997pt"/></p>

<img alt="$H_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/44e94a6264a39112e2400fca0e72c64d.svg?invert_in_darkmode" align=middle width="22.127325000000003pt" height="22.381919999999983pt"/> is [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix), that is <img alt="$v^T H_\alpha \left ( x \right ) v \ge 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/713d2862a036773717dac55d1df6ded8.svg?invert_in_darkmode" align=middle width="108.11097pt" height="27.598230000000008pt"/> for all <img alt="$v$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6c4adbc36120d62b98deef2a20d5d303.svg?invert_in_darkmode" align=middle width="8.52588pt" height="14.102549999999994pt"/>. Why? We have already seen that <img alt="$A^T A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/754dfb06e94eb52ca31296f2efb3ecf0.svg?invert_in_darkmode" align=middle width="34.921095pt" height="27.598230000000008pt"/> only has non-negative eigenvalues, so it is positive semidefinite by definition and <img alt="$\alpha I$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/25f90df9af50d0763ec4815d18611bb0.svg?invert_in_darkmode" align=middle width="19.021365000000003pt" height="22.381919999999983pt"/> is trivially positive definite for <img alt="$\alpha &gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/027fadd6000dcab00d8ea8a8007ce450.svg?invert_in_darkmode" align=middle width="40.59561pt" height="21.10812pt"/>. The sum of the two is positive definite again.  Thus <img alt="$c$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width="7.087278000000003pt" height="14.102549999999994pt"/> is a [strictly convex function](https://en.wikipedia.org/wiki/Convex_function). This is along to lines of the one-dimensional case: when the second derivative is <img alt="$&gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/64f8795af804d88316ed5c629b8eb159.svg?invert_in_darkmode" align=middle width="25.492335000000004pt" height="21.10812pt"/> everywhere, the function is strictly convex. Convex functions have a global minimum and, for strictly convex functions, this global minimum is unique. So we know there is only exactly one point that minimizes the cost function <img alt="$c_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/45300ff095a52c14741ae14e03807926.svg?invert_in_darkmode" align=middle width="15.601245pt" height="14.102549999999994pt"/>.

We can determine this global minimum <img alt="$x^*_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6ec6885a6e2b7f5a1f2d9652206f8961.svg?invert_in_darkmode" align=middle width="17.873955000000002pt" height="22.598730000000007pt"/> by solving <img alt="$\nabla c_\alpha \left ( x \right ) = 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/b16bcd0b4eef412c07429c94036334c0.svg?invert_in_darkmode" align=middle width="85.081095pt" height="24.56552999999997pt"/>:

<p align="center"><img alt="$$ \begin{align*} &#10;&amp; &amp; \nabla c_\alpha\left( x\right) &amp;= 0 \\&#10;&amp; \Leftrightarrow &amp; A^{T}Ax-A^{T}b+ax&amp;=0\\&#10;&amp; \Leftrightarrow &amp;\left( A^{T}A+\alpha I\right) x&amp;=A^{T}b\\&#10;&amp; \Leftrightarrow &amp; x&amp;=\left( A^{T}A+\alpha I\right) ^{-1}A^{T}b\end{align*}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/0a664e10561c1585b96454a0a29cdf51.svg?invert_in_darkmode" align=middle width="326.48055pt" height="102.74714999999999pt"/></p>

This is exactly the definition of the pseudoinverse without the limit. So we have found:

<p align="center"><img alt="$$&#10;\DeclareMathOperator*{\argmin}{arg\,min} &#10;x^*_{\alpha} = \argmin _{x}\left| \left| Ax-b\right| \right| _{2}^{2}+\alpha \left\| x\right\| _{2}^{2} = \left( A^{T}A+\alpha I\right) ^{-1}A^{T}b&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/f9b6d7757be95236664e77e894ad445c.svg?invert_in_darkmode" align=middle width="416.9847pt" height="23.656545pt"/></p>

with <img alt="$c_\alpha \left (x^*_\alpha \right ) \leq c_\alpha \left ( x \right )$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/bb41baf8e61121404e16575fef08cb69.svg?invert_in_darkmode" align=middle width="113.92903499999998pt" height="24.56552999999997pt"/> for all <img alt="$x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width="9.359955000000003pt" height="14.102549999999994pt"/>, and <img alt="$x^*_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6ec6885a6e2b7f5a1f2d9652206f8961.svg?invert_in_darkmode" align=middle width="17.873955000000002pt" height="22.598730000000007pt"/> denotes the minimum point.

This expression is continuous in <img alt="$\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width="10.537065000000004pt" height="14.102549999999994pt"/>, so we can take the limit <img alt="$\alpha \searrow 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/79a74cf1decc084acdd4d08b5ead7346.svg?invert_in_darkmode" align=middle width="44.235015pt" height="22.745910000000016pt"/>. Remember, we need the constraint of <img alt="$\alpha &gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/027fadd6000dcab00d8ea8a8007ce450.svg?invert_in_darkmode" align=middle width="40.59561pt" height="21.10812pt"/> for <img alt="$A^TA + \alpha I$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c359ea871a95ed98e8aef0c991b897c1.svg?invert_in_darkmode" align=middle width="73.986pt" height="27.598230000000008pt"/> to be invertible, so we can only take the limit from above. Taking the limit, we obtain:

<p align="center"><img alt="$$&#10;\DeclareMathOperator*{\argmin}{arg\,min} &#10;x^*= \lim_{x \searrow 0} \left( A^{T}A+\alpha I\right) ^{-1}A^{T}b&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/ed5e6f26f90f9d36f133ed81631ccf64.svg?invert_in_darkmode" align=middle width="273.84225pt" height="30.933870000000002pt"/></p>

with <img alt="$c \left (x^* \right ) \leq c \left ( x \right )$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/68b8018bced2fd3698e665339adda52d.svg?invert_in_darkmode" align=middle width="93.382575pt" height="24.56552999999997pt"/> for all <img alt="$x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width="9.359955000000003pt" height="14.102549999999994pt"/> with

<p align="center"><img alt="$$c \left ( x \right ) = \lim_{\alpha \searrow 0} \left\| Ax-b\right\| _{2}^{2}+\alpha \left\| x\right\| _{2}^{2} = \left\| Ax-b\right\| _{2}^{2}.$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/f0352206ee48174a22161f8ef60ecbfc.svg?invert_in_darkmode" align=middle width="325.8717pt" height="28.61232pt"/></p>

What do we gain from this? Well for one, we now know that this expression minimizes <img alt="$\left| \left| Ax-b\right| \right| _{2}^{2}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/478c3f5e8eebdac56c3ce2f070bbf9fa.svg?invert_in_darkmode" align=middle width="73.662765pt" height="31.305449999999972pt"/>. Furthermore, if there is a solution for <img alt="$A x =b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/eca79c671f72bf936a92296b8d7bc20d.svg?invert_in_darkmode" align=middle width="50.540985000000006pt" height="22.745910000000016pt"/>, we can easily use a similar approach to see that the solution <img alt="$x^*$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width="16.070010000000003pt" height="22.598730000000007pt"/> is smaller under Euclidean norm than any (other) solution <img alt="$\hat x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/a329fa3885326867a04979a16aea3922.svg?invert_in_darkmode" align=middle width="9.359955000000003pt" height="22.745910000000016pt"/> for <img alt="$Ax=b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width="50.540985000000006pt" height="22.745910000000016pt"/>.

We do this in two steps. First, we observe that

<p align="center"><img alt="$$A\hat x = b \Leftrightarrow A \hat x - b = 0 \Leftrightarrow \left\| A\widehat {x}-b\right\| _{2}^{2} = 0 $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/59c9fd30e1706e5b8c061d5d5df8c005.svg?invert_in_darkmode" align=middle width="283.1433pt" height="20.584245pt"/></p>

and, because  <img alt="$x^*_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6ec6885a6e2b7f5a1f2d9652206f8961.svg?invert_in_darkmode" align=middle width="17.873955000000002pt" height="22.598730000000007pt"/> minimizes <img alt="$c_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/45300ff095a52c14741ae14e03807926.svg?invert_in_darkmode" align=middle width="15.601245pt" height="14.102549999999994pt"/>,

<p align="center"><img alt="$$&#10;\begin{align*}&#10;&amp; &amp; c_{\alpha}\left( x_{\alpha }^{*}\right) &amp;\leq c_{\alpha }\left( \hat {x}\right) \\&#10;&amp; \Leftrightarrow &amp; c_{\alpha}\left( x_{\alpha }^{*}\right) &amp;\leq \left\| A\hat {x}-b\right\| _{2}^{2}+\left\| \hat {x}\right\| _{2}^{2}=0+\alpha \left\| \hat x\right\| _{2}^{2} \\&#10;&amp; \Leftrightarrow &amp; c_{\alpha}\left( x_{\alpha }^{*}\right) &amp; \leq \alpha \left\| \hat x\right\| _{2}^{2} \end{align*}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/b3feec3349fd1f1e89d33bb5860b7733.svg?invert_in_darkmode" align=middle width="327.19005pt" height="73.561455pt"/></p>

This expression is again continuous, so we can take the limit <img alt="$\alpha \searrow 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/79a74cf1decc084acdd4d08b5ead7346.svg?invert_in_darkmode" align=middle width="44.235015pt" height="22.745910000000016pt"/>, and we see:

<p align="center"><img alt="$$\begin{align*}&#10;&amp; c \left( x^{*} \right) \leq 0 \\&#10;\Leftrightarrow &amp;  \left\| Ax^{*}-b\right\| _{2}^{2} \le 0&#10;\end{align*}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/f8eaae9045e25fe18de6431636a35b27.svg?invert_in_darkmode" align=middle width="129.47616pt" height="45.38787pt"/></p>

Because norms are always non-negative, we have <img alt="$0 \le \left\| Ax^{*}-b\right\| _{2}^{2} \le 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/742dbe1154b028ff37bffc3e777db515.svg?invert_in_darkmode" align=middle width="140.35642499999997pt" height="31.305449999999972pt"/>, so <img alt="$\left\| Ax^{*}-b\right\| _{2}^{2} = 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/76d8fd0086a77b3d780c273428857340.svg?invert_in_darkmode" align=middle width="110.298045pt" height="31.305449999999972pt"/>. And we have observed above that this is equivalent to <img alt="$Ax^{*} = b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6578a90e2b55ae8ac1441b71ab954a55.svg?invert_in_darkmode" align=middle width="58.097985pt" height="22.745910000000016pt"/>. So if there is at least one exact solution to the problem, we are sure to obtain an exact one, too. To be fair, we could have deduced this in the previous section. However, we can take another limit on the inequality <img alt="$c_{\alpha}\left( x_{\alpha }^{*}\right) \leq \alpha \left\| \hat x\right\| _{2}^{2}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/42d83704d505af6bec19b7a10e3558da.svg?invert_in_darkmode" align=middle width="118.25121pt" height="31.305449999999972pt"/> and obtain a more interesting result. This time, we only take the limit <img alt="$\alpha \searrow 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/79a74cf1decc084acdd4d08b5ead7346.svg?invert_in_darkmode" align=middle width="44.235015pt" height="22.745910000000016pt"/> of <img alt="$x^{*}_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/457beca4b1e05450710bdb3b7a929a33.svg?invert_in_darkmode" align=middle width="17.873955000000002pt" height="22.598730000000007pt"/>, but keep <img alt="$c_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/45300ff095a52c14741ae14e03807926.svg?invert_in_darkmode" align=middle width="15.601245pt" height="14.102549999999994pt"/> fixed:

<p align="center"><img alt="$$&#10;\begin{align*}&#10;&amp;&amp;c_{\alpha}\left( x_{\alpha }^{*}\right) &amp;\leq \alpha \left\| \hat x\right\| _{2}^{2} \\&#10;\Rightarrow  &amp;&amp; c_{\alpha}\left( \lim_{\alpha \searrow 0} x_{\alpha }^{*}\right) &amp;\leq \alpha \left\| \hat x\right\| _{2}^{2}  \\&#10;\Leftrightarrow &amp;&amp; c_{\alpha}\left( x^{*} \right) &amp;\leq \alpha \left\| \hat x\right\| _{2}^{2} \\&#10;\Leftrightarrow &amp;&amp; \left\| Ax^{*}-b\right\| _{2}^{2}+\alpha \left\| x^{*}\right\| _{2}^{2} &amp;\leq \alpha \left\| \hat x\right\| _{2}^{2} \\&#10;\Rightarrow &amp;&amp; \alpha \left\| x^{*}\right\| _{2}^{2} &amp;\leq \alpha \left\| \hat x\right\| _{2}^{2} \\&#10;\Leftrightarrow &amp;&amp; \left\| x^{*}\right\| _{2}^{2} &amp;\leq \left\| \hat x\right\| _{2}^{2} &#10;\end{align*}&#10;$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/f36911a985a63c6c14c45a4325a958ec.svg?invert_in_darkmode" align=middle width="254.79629999999997pt" height="179.30715pt"/></p>

Here, we use <img alt="$\left\| Ax^{*}-b\right\| _{2}^{2} \ge 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/0a060abfd2c62cb3f1b72333c7bf7d65.svg?invert_in_darkmode" align=middle width="110.298045pt" height="31.305449999999972pt"/> to be able to drop the term and <img alt="$\alpha &gt; 0$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/027fadd6000dcab00d8ea8a8007ce450.svg?invert_in_darkmode" align=middle width="40.59561pt" height="21.10812pt"/> to preserve the direction of the inequality, and we obtain the second property about the length of <img alt="$\left \| x^{*} \right \|$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/582e4f4a0db551cfd4e0e64ae4ea62ed.svg?invert_in_darkmode" align=middle width="33.324720000000006pt" height="24.56552999999997pt"/>.

Now we have proved both properties that were mentioned in the book. On the one hand, if there are solutions, we will obtain one with minimal norm. On the other hand, if there are no solutions, using the pseudoinverse will provide us with an <img alt="$x$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width="9.359955000000003pt" height="14.102549999999994pt"/> that at least minimizes <img alt="$\left| \left| Ax-b\right| \right| _{2}^{2}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/478c3f5e8eebdac56c3ce2f070bbf9fa.svg?invert_in_darkmode" align=middle width="73.662765pt" height="31.305449999999972pt"/>.

To convince yourself that the expressions above are indeed continuous, we can use the technical argument that we can rewrite <img alt="$c_\alpha \left (x \right )$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/d31f2da0186af0176b32b9bfe6f111e1.svg?invert_in_darkmode" align=middle width="41.316165000000005pt" height="24.56552999999997pt"/> into <img alt="$c \left (\alpha, x \right )$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/6b152df936839351fd9f95828f44c480.svg?invert_in_darkmode" align=middle width="49.774395pt" height="24.56552999999997pt"/> and see that it is a continuous function in two variables instead of going the route of using functional analysis and treating <img alt="$c_\alpha$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/45300ff095a52c14741ae14e03807926.svg?invert_in_darkmode" align=middle width="15.601245pt" height="14.102549999999994pt"/> as a convergent series of functions.

## Broadcasting notation: oh my...

The broadcasting notation in the Deep Learning book is weird. For one moment, let's ignore its origins from [numpy](https://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html), and let's look at what's happening.

> [In the context of deep learning, we also use some less conventional notation. We allow the addition of matrix and a vector, yielding another matrix: <img alt="$C=A+b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/a058868c1eeeb7c29116ecaee01a9e63.svg?invert_in_darkmode" align=middle width="74.105295pt" height="22.745910000000016pt"/>, where <img alt="$C_{i,j}=A_{i,j}+b_j$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/9648c61543231b45bb1e310eb2d50319.svg?invert_in_darkmode" align=middle width="109.973985pt" height="22.745910000000016pt"/>. In other words, the vector <img alt="$b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width="7.028488500000004pt" height="22.745910000000016pt"/> is added to each row of the matrix. This shorthand eliminates the need to deÔ¨Åne a matrix with <img alt="$b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width="7.028488500000004pt" height="22.745910000000016pt"/> copied into each row before doing the addition. This implicit copying of <img alt="$b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width="7.028488500000004pt" height="22.745910000000016pt"/> to many locations is called broadcasting.](http://www.deeplearningbook.org/contents/linear_algebra.html#pf4)

Let's say <img alt="$A \in \mathbb{R}^{3 \times 3}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/3360d9edd908bcdacda81af5631dc947.svg?invert_in_darkmode" align=middle width="67.452495pt" height="26.70657pt"/> and <img alt="$b \in \mathbb{R}^3$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/2708f5c178cf45532e0d554174a6f27e.svg?invert_in_darkmode" align=middle width="45.434565000000006pt" height="26.70657pt"/>, for example:

<p align="center"><img alt="$$ A = \left[\begin{matrix}0 &amp; 0 &amp; 0\\1 &amp; 1 &amp; 1\\2 &amp; 2 &amp; 2\end{matrix}\right] , b = \left[\begin{matrix}1\\2\\3\end{matrix}\right] $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/91a679a3757c2355fe9d41938215b98c.svg?invert_in_darkmode" align=middle width="182.62695pt" height="59.068185pt"/></p>

Then, with broadcasting:

<p align="center"><img alt="$$ A + b = \left[\begin{matrix}1 &amp; 2 &amp; 3\\2 &amp; 3 &amp; 4\\3 &amp; 4 &amp; 5\end{matrix}\right] $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/aafb0437f2e8b2a024f91b5c930cd5b1.svg?invert_in_darkmode" align=middle width="140.63577pt" height="59.068185pt"/></p>

How do we get there? Essentially, we take the vector <img alt="$b \in \mathbb{R}^3$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/2708f5c178cf45532e0d554174a6f27e.svg?invert_in_darkmode" align=middle width="45.434565000000006pt" height="26.70657pt"/>, interpret it as a row vector, and add it to every row of the matrix, so:

<p align="center"><img alt="$$ A + b= A + \left[\begin{matrix}1\\1\\1\end{matrix}\right] b^T $$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/278d2dec780e58235994be81a048dc12.svg?invert_in_darkmode" align=middle width="142.953855pt" height="59.068185pt"/></p>

Wouldn't it make more sense to write this as <img alt="$A + b^T$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/173ab9c860562c7895c21c9ff676f383.svg?invert_in_darkmode" align=middle width="48.852705pt" height="27.598230000000008pt"/>? The reason for the unintuitive notation lies in the details of broadcasting: whereas in maths, a vector <img alt="$\mathbb{R}^3$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/d03c1e146df015e061405cc425738d83.svg?invert_in_darkmode" align=middle width="18.35592pt" height="26.70657pt"/> is identified as a column matrix <img alt="$\mathbb{R}^{3 \times 1}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/8734c499e46eb1c8c86d075d882041e3.svg?invert_in_darkmode" align=middle width="35.11959pt" height="26.70657pt"/> , in the context of broadcasting it is treated as a row matrix <img alt="$\mathbb{R}^{1 \times 3}$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/c8da33bcdd60c0bc68b7601f70ace233.svg?invert_in_darkmode" align=middle width="35.11959pt" height="26.70657pt"/>. This row matrix is then repeated across the <img alt="$1$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width="8.188554000000002pt" height="21.10812pt"/> dimension to make it into a <img alt="$3 \times 3$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/9f2b6b0a7f3d99fd3f396a1515926eb3.svg?invert_in_darkmode" align=middle width="36.420449999999995pt" height="21.10812pt"/> matrix that can be applied to <img alt="$A$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width="12.282765000000003pt" height="22.381919999999983pt"/>.  However, for matrix multiplications, <img alt="$b$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width="7.028488500000004pt" height="22.745910000000016pt"/> continues to be treated as column matrix. This is really confusing. So what's the reason for this ambiguity?

My best guess is that in ML contexts, data entries (examples) are often treated as row vectors. For example, a design matrix stores a number of examples, each in a separate row. So if you want to change the bias of your examples, you want to add a bias vector as row-vector to each row. Indeed, in chapter 8, broadcasting is used to express batch normalization in eq (8.35):

[<p align="center"><img alt="$$H'=\dfrac{H-\mu}{\sigma}$$" src="https://rawgit.com/blackhc/dlb_chapter2/master/svgs/fb90ffbca5d2c4578855bab28231f827.svg?invert_in_darkmode" align=middle width="88.399245pt" height="33.5874pt"/></p>](http://www.deeplearningbook.org/contents/optimization.html#pf2d)

I don't think this excuses the lack of clarity introduced with this notation, but then again it is too late to change and make everybody use a transposed design matrix... :) 

## Looking back and ahead

There is much more I could write about, but I think these were the most interesting bits from my notes. We have revisited convex functions, continuity and limits to motivate the curious definition of the Moore-Penrose pesudoinverse. Last but not least, we have looked at the origin of the broadcasting notation and why it might be confusing to someone who is new to ML. Next up: reading Chapter 3 and thinking about probability theory.

Stay tuned, \
 Andreas

PS: Thanks to my friend Armin Krupp for suggestions and corrections of the draft. All current mistakes and factual errors have been added later.

PSS: This a gist repost of a post on my blog http://blog.blackhc.net/2017/03/dlb-chapter2/index.html. I wish Medium was supporting LaTeX formulas properly...